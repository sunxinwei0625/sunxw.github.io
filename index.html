<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Xinwei Sun</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Xinwei Sun</h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://sunxw.github.io/"><img src="picture/wanzi_merry.jpg" alt="alt text" width="120px" /></a>&nbsp;</td>
<a align="justify" style=font-size:20px href="https://sds.fudan.edu.cn/">Fudan University, School of Data Science</a><br />
<p align="justify" style=font-size:20px> Phone: +86 13718916343 </a></p>
<p align="justify" style=font-size:20px> Email: sunxinwei@fudan.edu.cn </a></p> 
<br />
</td></tr></table>
<h2>About me</h2>
<p align="justify" style=font-size:20px> I am now a tenure-track assistant professor in the School of Data Science, at Fudan University. I received Ph.D. in Statistics at the School of Mathematical Science, Peking University, advised by <a href="https://yao-lab.github.io/">Yuan Yao</a> and <a href="https://cfcs.pku.edu.cn/english/people/faculty/yizhouwang/index.htm">Yizhou Wang</a>.</p> 
<p align="justify" style=font-size:20px> As a statistician, I'm on a continuous journey that intertwines statistics with a wide range of applications, including Neuroimaging and Artificial Intelligence. My commitment lies in bridging the gap between statistical methods and real-world challenges. I achieve this by immersing myself in understanding the intricacies of these applications, gaining domain-specific expertise, and weaving it into the refinement of more potent statistical theories.

<div style="display: inline-block; vertical-align: top; margin-right: 20px;">
    <div style="width: 150px; height: 100px; border: 2px solid #000; background-color: #f2f2f2; text-align: center;">
        <p>Left Box</p>
    </div>
</div>

<svg width="100" height="100" style="vertical-align: top;">
    <!-- Define an arrowhead marker -->
    <defs>
        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto">
            <polygon points="0 0, 10 3.5, 0 7" fill="black" />
        </marker>
    </defs>

    <!-- Line with arrowhead -->
    <line x1="0" y1="50" x2="50" y2="50" stroke="black" stroke-width="2" marker-end="url(#arrowhead)" />
</svg>

<div style="display: inline-block; vertical-align: top;">
    <div style="width: 150px; height: 100px; border: 2px solid #000; background-color: #f2f2f2; text-align: center;">
        <p>Right Box</p>
    </div>
</div>

< br/>
< br/>

<div style="display: inline-block; vertical-align: top;">
    <div style="width: 150px; height: 100px; border: 2px solid #000; background-color: #f2f2f2; text-align: center;">
        <p>down Box</p>
    </div>
</div>

  
<h2>Education & Research Stay</h2>
<p style=font-size:20px> 2019-2022  Researcher. Microsoft Research Asia (Machine Learning group)</a>.</p>
<p style=font-size:20px> 2013-2018  Ph.D.  School of Mathematical Sciences at Peking University</a>.</p>
<p style=font-size:20px> 2009-2013  B.S.   School of Mathematical Sciences at Nankai University</a>.</p>
<br />
<h2>Teaching</h2>
<p style=font-size:20px> 2023 Fall,  Advanced Statistical Theory</a>.</p>
<p style=font-size:20px> 2023 Spring,  Advanced Statistical Theory</a>.</p>
<p style=font-size:20px> 2023 Spring,  Mathematical Statistics</a>.</p>
<p style=font-size:20px> 2022 Fall,  Advanced Regression Analysis</a>.</p>
<br />
<h2>Publications</h2> <p>(*Co-first Author/Alphabetic Order #Corresponding Author)</p>
<ul>
<h2>Sparsity Learning and Statistical Inference</h2>
<ul>
<li><p align="justify" style=font-size:18px>Split Knockoffs for Multiple Comparisons: Controlling the Directional False Discovery Rate. </a> <br />
Yang Cao*, <b>Xinwei Sun*#</b>, Yuan Yao*. <br />
<i> Accepted by Journal of the American Statistical Association <b>(JASA)</b>, 2023.</p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="https://arxiv.org/abs/2103.16159">Controlling the False Discovery Rate in Transformational Sparsity: Split Knockoffs.</a> <br />
Yang Cao*, <b>Xinwei Sun*#</b>, Yuan Yao*. <br />
<i> Accepted by Journal of the Royal Statistical Society: Series B <b>(JRSSB)</b>, 2023.</p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="https://www.sciencedirect.com/science/article/pii/S1063520318300010">Boosting with Structural Sparsity: A Differential Inclusion Approach.</a> <br />
Chendi Huang*, <b>Xinwei Sun*</b>, Jiechao Xiong*, Yuan Yao*#. <br/>
<i>Applied and Computational Harmonic Analysis. <b>(ACHA)</b>, 2020.</p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="https://ieeexplore.ieee.org/abstract/document/9187891/">Perturbed Amplitude Flow for Phase Retrieval.</a> <br />
Bing Gao*, <b>Xinwei Sun*</b>, Yang Wang*#, Zhiqiang Xu*. <br/>
<i>IEEE Transactions on Signal Processing. <b>(IEEE TSP)</b>, 2020.</p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="https://dl.acm.org/doi/abs/10.1145/3603165.3607367">Sparse Learning in AI: A Differential Inclusion Perspective.</a> <br />
<b>Xinwei Sun*</b>. <br/>
<i>Proceedings of the ACM Turing Award Celebration Conference-China 2023.</p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="https://arxiv.org/abs/2301.00545">Knockoffs-SPR: Clean Sample Selection in Learning with Noisy Labels.</a> <br />
Yikai Wang*, Yanwei Fu*,<b>Xinwei Sun#</b>. <br/>
<i>Major revision requested at <b>(IEEE TPAMI)</b>.</p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="https://arxiv.org/abs/1905.09449">Exploring Structural Sparsity of Deep Networks via Inverse Scale Spaces.</a> <br />
Yanwei Fu, Chen Liu, Donghao Li, Zuyuan Zhong, Xinwei Sun, Jinshan Zeng, and Yuan Yao#. <br />
<i>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(IEEE TPAMI)</b>, 2023.</p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="http://webcache.googleusercontent.com/search?q=cache:SO4uNurgiHsJ:proceedings.mlr.press/v119/fu20d/fu20d.pdf+&cd=4&hl=zh-CN&ct=clnk&gl=hk">DessiLBI: Exploring Structural Sparsity of Deep Networks via Differential Inclusion Paths.</a> <br />
Yanwei Fu, Chen Liu, Donghao Li, Zuyuan Zhong, <b>Xinwei Sun</b>, Jinshan Zeng#, Yuan Yao#. <br/>
<i>International Conference on Machine Learning <b>(ICML)</b>, 2020.</p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="https://arxiv.org/abs/2203.07788">Scalable Penalized Regression for Noise Detection in Learning with Noisy Labels.</a> <br />
Yikai Wang, <b>Xinwei Sun</b>, Yanwei Fu#. <br />
<i>IEEE Computer Society Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b>, 2022. </p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="https://webcache.googleusercontent.com/search?q=cache:QdW-9vRqEW4J:https://proceedings.neurips.cc/paper/2019/hash/333ac5d90817d69113471fbb6e531bee-Abstract.html+&cd=2&hl=zh-CN&ct=clnk&gl=hk">iSplit LBI: Individualized Partial Ranking with Ties via Split LBI.</a> <br />
Qianqian Xu, <b>Xinwei Sun</b>, Zhiyong Yang, Xiaochun Cao, Qingming Huang, Yuan Yao. <br/>
<i>Annual Conference on Neural Information Processing Systems <b>(NeurIPS)</b>, 2019.</p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="http://proceedings.mlr.press/v80/zhao18c.html">MSplit LBI: Realizing Feature Selection and Dense Estimation in Few-shot and Zero-shot Learning.</a> <br />
<b>Xinwei Sun*</b>, Bo Zhao*, Yanwei Fu#, Yuan Yao#, Yizhou Wang. <br/>
<i>International Conference on Machine Learning <b>(ICML)</b>, 2018.</p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="https://webcache.googleusercontent.com/search?q=cache:k5pToxsJGXQJ:https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480171.pdf+&cd=3&hl=zh-CN&ct=clnk&gl=hk">TCGM:An Information-Theoretic Framework for Semi-Supervised Multi-Modality Learning.</a> <br />
<b>Xinwei Sun*</b>, Yilun Xu*, Peng Cao, Yuqing Kong#, Lingjing Hu, Shanghang Zhang#, Yizhou Wang. <br />
<i>European Conference on Computer Vision <b>(ECCV Oral)</b>, 2020. </p>
</li>
</ul>
<ul> 
<li><p style=font-size:18px><a href="https://webcache.googleusercontent.com/search?q=cache:UnvkZ3EBCAEJ:https://proceedings.neurips.cc/paper/2016/hash/2451041557a22145b3701b0184109cab-Abstract.html+&cd=3&hl=zh-CN&ct=clnk">Split LBI: An Iterative Regularization Path with Structural Sparsity.</a> <br />
Chendi Huang*, <b>Xinwei Sun*</b>, Jiechao Xiong*, Yuan Yao*#. <br/>
<i>Advances in Neural Information Processing Systems <b>(NeurIPS)</b>, 2016.</p>
</li>
</ul>
<ul> 
<li><p style=font-size:18px><a href="https://link.springer.com/chapter/10.1007/978-3-030-00928-1_69">FDR-HS: An Empirical Bayesian Identification of Heterogenous Features in Neuroimage Analysis.</a> <br />
<b>Xinwei Sun</b>, Lingjing Hu#, Yuan Yao#, Yizhou Wang. <br />
<i>Medical Image Computing and Computer Assisted Interventions Conference. <b>(MICCAI)</b>, 2018. </p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="springerprofessional.de/en/gsplit-lbi-taming-the-procedural-bias-in-neuroimaging-for-diseas/14978580">GSplit LBI: Taming the Procedural Bias in Neuroimaing for Disease Prediction.</a> <br />
<b>Xinwei Sun</b>, Lingjing Hu#, Yuan Yao#, Yizhou Wang. <br />
<i>Medical Image Computing and Computer Assisted Interventions Conference. <b>(MICCAI)</b>, 2017. </p>
</li>
</ul>
<h2>Causal Learning and Out-Of-Distribution Generalization</h2>
<ul>
<li><p style=font-size:18px><a href="https://arxiv.org/abs/2305.05276">Causal Discovery from Subsampled Time Series with Proxy Variables.</a> <br />
Mingzhou Liu, <b>Xinwei Sun#</b>, Lingjing Hu, Yizhou Wang. <br />
<i>Conference on Neural Information Processing Systems <b>(NeurIPS)</b>, 2023. </p>
</li>
</ul>
<ul>
<li><p style=font-size:18px>Which Invariance should we Transfer? A Causal Minimax Learning Approach.</a> <br />
Mingzhou Liu, Xiangyu Zheng, <b>Xinwei Sun#</b>, Fang Fang, Yizhou Wang. <br />
<i>International Conference on Machine Learning <b>(ICML)</b>, 2023.<a href="pdf/which_invariance_causality.pdf">[PDF]</a></i></p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="https://proceedings.neurips.cc/paper/2021/file/8c6744c9d42ec2cb9e8885b54ff744d0-Paper.pdf">Recovering Latent Causal Factor for Generalization to Distributional Shifts.</a> <br />
<b>Xinwei Sun#</b>, Botong Wu, Xiangyu Zheng, Chang Liu, Wei Chen, Tao Qin, Tie-Yan Liu. <br />
<i>Conference on Neural Information Processing Systems <b>(NeurIPS)</b>, 2021. </p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="https://papers.nips.cc/paper/2021/hash/310614fca8fb8e5491295336298c340f-Abstract.html">Learning Causal Semantic Representation for Out-of-Distribution Prediction.</a> <br />
Chang Liu#, <b>Xinwei Sun</b>, JindongWang, Haoyue Tang, Tao Li, Tao Qin, Wei Chen, Tie-Yan Liu. <br />
<i>Conference on Neural Information Processing Systems <b>(NeurIPS)</b>, 2021. </p> 
</li> 
</ul>
<ul>
<li><p style=font-size:18px>A New Causal Decomposition Paradigm towards Health Equity.</a> <br />
<b>Xinwei Sun#</b>, Xiangyu Zheng, Jim Weinstein. <br />
<i>International Conference on Artificial Intelligence and Statistics <b>(AISTATS)</b>, 2023.<a href="pdf/147.pdf">[PDF]</a></i></p>
</li>
</ul>
<ul>  
<li><p style=font-size:18px><a href="https://openreview.net/forum?id=-HHJZlRpGb">Learning Domain-Agnostic Representation for Disease Diagnosis.</a> <br />
Chu-ran Wang, Jing Li, <b>Xinwei Sun#</b>, Fandong Zhang, Yizhou Yu, Yizhou Wang. <br />
<i>International Conference on Learning Representations <b>(ICLR)</b>, 2023. </p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="https://openreview.net/forum?id=gUZWOE42l6Q">Out-of-distribution Representation Learning for Time Series Classification.</a> <br />
Wang Lu, Jindong Wang, <b>Xinwei Sun</b>, Yiqiang Chen, Xing Xie. <br />
<i>International Conference on Learning Representations <b>(ICLR)</b>, 2023. </p>
</li> 
</ul>
<ul>
<li><p style=font-size:18px><a href="https://ieeexplore.ieee.org/abstract/document/9956886">PatchMix Augmentation to Identify Causal Features in Few-shot Learning.</a> <br />
Chengming Xu*, Chen Liu*, <b>Xinwei Sun#</b>, Siqian Yang, Yabiao Wang, Chengjie Wang, Yanwei Fu#. <br />
<i>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(IEEE TPAMI)</b>, 2022. </p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="https://ieeexplore.ieee.org/abstract/document/9540781">Bilateral Asymmetry Guided Counterfactual Generating Network for Mammogram Classification.</a> <br />
Churan Wang*, Jing Li*, Fandong Zhang, <b>Xinwei Sun#</b>, Hao Dong, Yizhou Wang#. <br />
<i>IEEE Transactions on Image Processing <b>(IEEE TIP)</b>, 2021. </p> 
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Causal_Hidden_Markov_Model_for_Time_Series_Disease_Forecasting_CVPR_2021_paper.pdf">Causal Hidden Markov Model for Time Series Disease Forecasting.</a> <br />
Jing Li, Botong Wu, <b>Xinwei Sun#</b>, Yizhou Wang. <br />
<i>IEEE Computer Society Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b>, 2021. </p>
</ul>
<h2>Applications.</h2>
<ul>
<li><p style=font-size:18px><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_Forecasting_Irreversible_Disease_via_Progression_Learning_CVPR_2021_paper.pdf">Disease Forecast via Progression Learning.</a> <br />
Botong Wu*, Sijie Ren*, Jing Li, <b>Xinwei Sun#</b>, Shiming Li, Yizhou Wang. <br />
<i>IEEE Computer Society Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b>, 2021. </p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="https://miccai2021.org/openaccess/paperlinks/2021/09/01/079-Paper0053.html">CA-Net: Leveraging Contextual Features for Lung Cancer Prediction.</a> <br />
Mingzhou Liu, Fandong Zhang, <b>Xinwei Sun#</b>, Yizhou Yu, Yizhou Wang. <br />
<i>Medical Image Computing and Computer Assisted Interventions Conference. <b>(MICCAI)</b>, 2021. </p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="https://link.springer.com/chapter/10.1007/978-3-030-87240-3_5">DAE-GCN: Identifying Disease-Related Features for Disease Prediction.</a> <br />
Chu-ran Wang, <b>Xinwei Sun#</b>, Fandong Zhang, Yizhou Yu, Yizhou Wang. <br />
<i>Medical Image Computing and Computer Assisted Interventions Conference. <b>(MICCAI)</b>, 2021. </p>
</li>
</ul>
<ul>
<li><p style=font-size:18px><a href="https://webcache.googleusercontent.com/search?q=cache:RmmCPJdD4p4J:https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Learning_With_Unsure_Data_for_Medical_Image_Diagnosis_ICCV_2019_paper.pdf+&cd=2&hl=zh-CN&ct=clnk">Learning with Unsure Data for Medical Image Diagnosis.</a> <br />
Botong Wu, <b>Xinwei Sun#</b>, Lingjing Hu, Yizhou Wang. <br />
<i>IEEE International Conference on Computer Vision <b>(ICCV)</b>, 2019. </p>
</li>
</ul>
<ul> 
<li><p style=font-size:18px><a href="https://ieeexplore.ieee.org/document/8953247">Cascaded Generative and Discriminative Learning for Microcalcification Detection in Breast Mammograms.</a> <br />
Fandong Zhang*, Ling Luo*, <b>Xinwei Sun</b>, Zhen Zhou, Xiuli Li, Yizhou Yu, Yizhou Wang. <br />
<i>IEEE Computer Society Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b>, 2019. </p>
</li>
</ul>
<ul> 
<li><p style=font-size:18px><a href="https://ieeexplore.ieee.org/document/8658396">Zero-shot Learning via Recurrent Knowledge Transfer.</a> <br />
Bo Zhao, <b>Xinwei Sun</b>, Xiaopeng Hong, Yuan Yao, Yizhou Wang. <br />
<i>IEEE Winter Conference on Applications of Computer Vision <b>(WACV)</b>, 2019. </p>
</li>
</ul>
<ul> 
<li><p style=font-size:18px><a href="https://dl.acm.org/doi/abs/10.1145/3240508.3240597">A Margin-based MLE for Crowdsourced Partial Ranking.</a> <br />
Qianqian Xu, Jiechao Xiong, <b>Xinwei Sun</b>, Zhiyong Yang, Xiaochun Cao, Qingming Huang, Yuan Yao. <br />
<i>ACM International Conference on Multimedia <b>(ACM-MM)</b>, 2018. </p>
</li>
</ul>
</ul>
<br/ >
<h2>Research</h2>
<ul>
<p style=font-size:20px> 1. Sparsity Learning and Statistical Inference.
<ul>
<h2> (a) Theory. </h2>
<ul>
<p style=font-size:18px align="justify"> (1) Controlling the False Discovery Rate in Transformational Sparsity: Split Knockoffs. <b>(JRSSB)</b>, 2023.</p></a> <br />
<img src="picture/split_simulation.png" alt="alt text" width="800px" /><br />
<br />
<p align="justify" style=font-size:16px> Controlling the False Discovery Rate (FDR) in a variable selection procedure is critical for reproducible discoveries, and it has been extensively studied in sparse linear models. However, it remains largely open in scenarios where the sparsity constraint is not directly imposed on the parameters but on a linear transformation of the parameters to be estimated. In this paper, we propose a data-adaptive FDR control method, called the \emph{Split Knockoff} method, for this transformational sparsity setting. The proposed method exploits both variable and data splitting. The linear transformation constraint is relaxed to its Euclidean proximity in a lifted parameter space, which yields an orthogonal design that enables the orthogonal Split Knockoff construction. To overcome the challenge that exchangeability fails due to the heterogeneous noise brought by the transformation, new inverse supermartingale structures are developed via data splitting for provable FDR control without sacrificing power. </p> 
</li> 
</ul> 
<ul>
<p style=font-size:18px align="justify"> (2) Split Knockoffs for Multiple Comparisons: Controlling the Directional False Discovery Rate. <b>(JASA)</b>, 2023.</p></a> <br />
<img src="picture/knockoff_sign_procedure.png" alt="alt text" width="800px" /> <br />
<br />
<p align="justify" style=font-size:16px> Multiple comparisons in hypothesis testing often encounter structural constraints in various applications. We propose an extended Split Knockoff method specifically designed to address the control of directional false discovery rate under linear transformations. Our proposed approach relaxes the stringent linear manifold constraint to its neighborhood, employing a variable splitting technique commonly used in optimization. This methodology yields an orthogonal design that benefits both power and directional false discovery rate control. By incorporating a sample splitting scheme, we achieve effective control of the directional false discovery rate, with a notable reduction to zero as the relaxed neighborhood expands. </p> 
</li> 
</ul> 
<ul>
<p style=font-size:18px align="justify"> (3) Boosting with Structural Sparsity: A Differential Inclusion Approach. <b>(ACHA)</b>, 2020.</p></a> <br />
<img src="picture/Split_theory.png" alt="alt text" width="800px" /> <br />
<br/>
<p align="justify" style=font-size:16px> Boosting as gradient descent algorithms is one popular method in machine learning. We propose a novel Boosting-type algorithm based on restricted gradient descent with structural sparsity control whose underlying dynamics are governed by differential inclusions. We present an iterative regularization path with structural sparsity where the parameter is sparse under some linear transforms, based on variable splitting and the Linearized Bregman Iteration. Hence it is called Split LBI. Despite its simplicity, Split LBI outperforms the popular generalized Lasso in both theory and experiments. A theory of path consistency is presented that equipped with a proper early stopping, Split LBI may achieve model selection consistency under a family of Irrepresentable Conditions which can be weaker than the necessary and sufficient condition for generalized Lasso. Furthermore, some 
\ell_2 error bounds are also given at the minimax optimal rates. </p> 
</li> 
</ul>
<ul>
<p style=font-size:18px align="justify"> (4) Perturbed Amplitude Flow for Phase Retrieval. <b>(TSP)</b>, 2020.</p></a> <br />
<img src="picture/PAF_galaxy.png" alt="alt text" width="800px" /> <br />
<br />
<p align="justify" style=font-size:16px> In this paper, we propose a new non-convex algorithm for solving the phase retrieval problem, The proposed algorithm solves a new proposed model, perturbed amplitude-based model, for phase retrieval, and is correspondingly named as Perturbed Amplitude Flow (PAF). We prove that PAF can recover cx (|c| = 1) under O(n) Gaussian random measurements (optimal order of measurements). Starting with a designed initial point, our PAF algorithm iteratively converges to the true solution at a linear rate for both real, and complex signals. Besides, PAF algorithm needn't any truncation or re-weighted procedure, so it enjoys simplicity for implementation.  </p> 
</li> 
</ul>
<ul>
<p style=font-size:18px align="justify"> (5) Split LBI: An Iterative Regularization Path with Structural Sparsity. <b>(NeurIPS)</b>, 2016.</p></a> <br />
<img src="picture/splitlbi.png" alt="alt text" width="800px" /> <br />
<br />
<p align="justify" style=font-size:16px> An iterative regularization path with structural sparsity is proposed in this paper based on variable splitting and the Linearized Bregman Iteration, hence called \emph{Split LBI}. Despite its simplicity, Split LBI outperforms the popular generalized Lasso in both theory and experiments. A theory of path consistency is presented that equipped with a proper early stopping, Split LBI may achieve model selection consistency under a family of Irrepresentable Conditions which can be weaker than the necessary and sufficient condition for generalized Lasso. Furthermore, some 
\ell_2 error bounds are also given at the minimax optimal rates. </p> 
</li> 
</ul>
<h2> (b) Applications (Neuroimaging and Deep Learning).</h2>
<ul>
<p style=font-size:18px align="justify"> (1) GSplit LBI: Taming the Procedural Bias in Neuroimaging for Disease Prediction. <b>(MICCAI)</b>, 2017.</p></a> <br />
<img src="picture/AD_GSplit.png" alt="alt text" width="800px" /> <br />
<br />
<p align="justify" style=font-size:16px> In voxel-based neuroimage analysis, lesion features have been the main focus in disease prediction due to their interpretability with respect to related diseases. However, we observe that there exists another type of features introduced during the preprocessing steps and we call them “Procedural Bias”. Besides, such bias can be leveraged to improve classification accuracy. Nevertheless, most existing models
suffer from either under-fit without considering procedural bias or poor interpretability without differentiating such bias from lesion ones. In this
paper, a novel dual-task algorithm namely GSplit LBI is proposed to
resolve this problem. By introducing an augmented variable enforced
to be structural sparsity with a variable splitting term, the estimators
for prediction and selecting lesion features can be optimized separately
and mutually monitored by each other following an iterative scheme.
Empirical experiments have been evaluated on Alzheimer’s Disease
Neuroimaging Initiative (ADNI) database. </p> 
</li> 
</ul> 
<ul>
<p style=font-size:18px align="justify"> (2) FDR-HS: An Empirical Bayesian Identification of Heterogenous Features in Neuroimage Analysis. <b>(MICCAI)</b>, 2018.</p></a> <br />
<img src="picture/fdrhs_b.png" alt="alt text" width="800px" /> <br />
<br />
<p align="justify" style=font-size:16px> Recent studies found that in voxel-based neuroimage analysis,
detecting and differentiating procedural bias" that is introduced
during the preprocessing steps from lesion features, not only help
boost accuracy but also can improve interpretability. To capture both procedural bias and lesion features, we propose
a two-group" Empirical-Bayes method called FDR-HS (False-
Discovery-Rate Heterogenous Smoothing). Such a method is able to not
only avoid multicollinearity but also exploit the heterogenous spatial
patterns of features. In addition, it enjoys simplicity in implementation
by introducing hidden variables, which turns the problem into a convex
optimization scheme and can be solved efficiently by the expectation maximization
(EM) algorithm.  </p> 
</li> 
</ul> 
<ul>
<p style=font-size:18px align="justify"> (3) DessiLBI: Exploring Structural Sparsity of Deep Networks via Differential Inclusion Paths. <b>(ICML)</b>, 2020.</p></a> <br />
<img src="picture/dessilbi.png" alt="alt text" width="800px" /> <br />
<br />
<p align="justify" style=font-size:16px> Over-parameterization is ubiquitous nowadays in
training neural networks to benefit both optimizations
in seeking global optima and generalization
in reducing prediction error. We propose a new
approach based on differential inclusions of inverse
scale-spaces. Specifically, it generates a
family of models from simple to complex ones
that couples a pair of parameters simultaneously
train over-parameterized deep models and
structural sparsity on weights of fully connected
and convolutional layers. Such a differential inclusion
the scheme has a simple discretization, proposed
as Deep structurally splitting Linearized
Bregman Iteration (DessiLBI), whose global convergence
analysis in deep learning is established
that from any initializations, algorithmic iterations
converge to a critical point of empirical risks. Experimental
evidence shows that DessiLBI achieve
comparable and even better performance than
the competitive optimizers in exploring the structural
the sparsity of several widely used backbones on
the benchmark datasets. Remarkably, with early
stopping, DessiLBI unveils “winning tickets” in
early epochs: the effective sparse structure with
comparable test accuracy to fully trained overparameterized
models. </p> 
</li> 
</ul>
</ul>
<br />
<p style=font-size:20px> 2. Causal Inference and Learning. 
<ul>
<p style=font-size:18px align="justify"> (1) Causal Discovery from Subsampled Time Series with Proxy Variables. <b>(NeurIPS)</b>, 2023.</p></a> <br />
<img src="picture/AD_causal_discovery.png" alt="alt text" width="800px" /><br />
<br />
<p align="justify" style=font-size:16px>  Inferring causal structures from time series data is the central interest of many scientific inquiries. We propose a constraint-based algorithm that can identify the entire causal structure from subsampled time series, without any parametric constraint. Our observation is that the challenge of subsampling arises mainly from hidden variables at the unobserved time steps. Meanwhile, every hidden variable has an observed proxy, which is essentially itself at some observable time in the future, benefiting from the temporal structure. Based on these, we can leverage the proxies to remove the bias induced by the hidden variables and hence achieve identifiability. Following this intuition, we propose a proxy-based causal discovery algorithm.  </p> 
</li> 
</ul> 
<ul>
<p style=font-size:18px align="justify"> (2) Which Invariance should we Transfer? A Causal Minimax Learning Approach. <b>(ICML)</b>, 2023.</p></a> <br />
<img src="picture/causal_minimax.png" alt="alt text" width="800px" /><br />
<br />
<p align="justify" style=font-size:16px> A major barrier to deploying current machine learning models lies in their non-reliability to dataset shifts. Despite recent advances, a key question remains: which subset of this whole stable information should the model transfer, in order to achieve optimal generalization ability? To answer this question, we present a comprehensive minimax analysis from a causal perspective. Specifically, we first provide a graphical condition for the whole stable set to be optimal. When this condition fails, we surprisingly find with an example that this whole stable set, although can fully exploit stable information, is not the optimal one to transfer. To identify the optimal subset under this case, we propose to estimate the worst-case risk with a novel \emph{optimization} scheme over the intervention functions on mutable causal mechanisms. We then propose an efficient algorithm to search for the subset with minimal worst-case risk, based on a newly defined equivalence relation between stable subsets.   </p> 
</li> 
</ul> 
<ul>
<p style=font-size:18px align="justify"> (3) Recovering Latent Causal Factor for Generalization to Distributional Shifts. <b>(NeurIPS)</b>, 2023.</p></a> <br />
<img src="picture/lacim.png" alt="alt text" width="800px" /><br />
<br />
<p align="justify" style=font-size:16px>  Distributional shifts between training and target domains may degrade the prediction accuracy of learned models, mainly because these models often learn features that possess only correlation rather than causal relation with the output. 
We propose a new method called LaCIM that specifies the underlying causal structure of the data and the source of distributional shifts, guiding us to pursue only causal factor for prediction. Specifically, the LaCIM introduces a pair of correlated latent factors: (a) causal factor and (b) others, while the extent of this correlation is governed by a domain variable that characterizes the distributional shifts. On the basis of this, we prove that the distribution of observed variables conditioning on latent variables is shift-invariant. Equipped with such an invariance, we prove that the causal factor can be recovered without mixing information from others, which induces the ground-truth predicting mechanism. We propose a Variational-Bayesian-based method to learn this invariance for prediction. </p> 
</li> 
</ul>
<ul>
<p style=font-size:18px align="justify"> (4) Learning Causal Semantic Representation for Out-of-Distribution Prediction. <b>(NeurIPS)</b>, 2021.</p></a> <br />
<img src="picture/causal_semantic_ood.png" alt="alt text" width="800px" /><br />
<br/>
<p align="justify" style=font-size:16px> Conventional supervised learning methods, especially deep ones, are found to be sensitive to out-of-distribution (OOD) examples, largely because the learned representation mixes the semantic factor with the variation factor due to their domain-specific correlation, while only the semantic factor causes the output. To address the problem, we propose a Causal Semantic Generative model (CSG) based on causal reasoning so that the two factors are modeled separately, and develop methods for OOD prediction from a single training domain, which is common and challenging. The methods are based on the causal invariance principle, with a novel design in variational Bayes for both efficient learning and easy prediction. Theoretically, we prove that under certain conditions, CSG can identify the semantic factor by fitting training data, and this semantic identification guarantees the boundedness of OOD generalization error and the success of adaptation. </p> 
</li> 
</ul>
<ul>
<p style=font-size:18px align="justify"> (5) Bilateral Asymmetry Guided Counterfactual Generating Network for Mammogram Classification. <b>(TIP)</b>, 2021.</p></a> <br />
<img src="picture/counterfactual_result.png" alt="alt text" width="800px" /><br />
<br/>
<p align="justify" style=font-size:16px> Mammogram benign or malignant classification with only image-level labels is challenging due to the absence of lesion annotations. We derive a new theoretical result based on counterfactual analysis to identify lesion areas without annotations. Specifically, by building a causal model that entails such a prior for bilateral images, we identify to optimize the distances in distribution between i) the counterfactual features and the target side’s features in lesion-free areas; and ii) the counterfactual features and the reference side’s features in lesion areas. We propose a counterfactual generative network for optimization. Our method can outperform baselines by 20% in Type-I error of lesion detection.  </p> 
</li> 
</ul>
</ul>
<div id="footer-text">
<br>Page generated 2023-06-18, by <a href="https://sunxinwei0625.github.io/sunxw.github.io/">Xinwei Sun</a>.
</div>
</div>
</div>
</body>
</html> 
